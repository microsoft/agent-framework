# Copyright (c) Microsoft. All rights reserved.

import asyncio
import sys
from abc import ABC, abstractmethod
from collections.abc import AsyncIterable, Callable, MutableMapping, MutableSequence, Sequence
from typing import TYPE_CHECKING, Any, ClassVar, Generic, Literal, Protocol, TypeVar, runtime_checkable

from pydantic import BaseModel

from ._logging import get_logger
from ._memory import ContextProvider
from ._middleware import (
    ChatMiddleware,
    ChatMiddlewareCallable,
    FunctionMiddleware,
    FunctionMiddlewareCallable,
    Middleware,
)
from ._serialization import SerializationMixin
from ._threads import ChatMessageStoreProtocol
from ._tools import FUNCTION_INVOKING_CHAT_CLIENT_MARKER, FunctionInvocationConfiguration, ToolProtocol
from ._types import (
    ChatMessage,
    ChatResponse,
    ChatResponseUpdate,
    ToolMode,
    prepare_messages,
)

if sys.version_info >= (3, 11):
    from typing import TypedDict
else:
    from typing_extensions import TypedDict

if sys.version_info >= (3, 12):
    from typing import Unpack  # pragma: no cover
else:
    from typing_extensions import Unpack  # pragma: no cover

if TYPE_CHECKING:
    from ._agents import ChatAgent
    from ._types import BaseChatOptionsDict


TInput = TypeVar("TInput", contravariant=True)
TEmbedding = TypeVar("TEmbedding")
TBaseChatClient = TypeVar("TBaseChatClient", bound="BaseChatClient")
TOptions = TypeVar(
    "TOptions", bound=TypedDict, default="BaseChatOptionsDict", covariant=True
)  # TypedDict options type for chat clients

logger = get_logger()

__all__ = [
    "BaseChatClient",
    "ChatClientProtocol",
]


# region ChatClientProtocol Protocol


@runtime_checkable
class ChatClientProtocol(Protocol[TOptions]):
    """A protocol for a chat client that can generate responses.

    This protocol defines the interface that all chat clients must implement,
    including methods for generating both streaming and non-streaming responses.

    The generic type parameter TOptions specifies which options TypedDict this
    client accepts, enabling IDE autocomplete and type checking for provider-specific
    options.

    Note:
        Protocols use structural subtyping (duck typing). Classes don't need
        to explicitly inherit from this protocol to be considered compatible.

    Examples:
        .. code-block:: python

            from agent_framework import ChatClientProtocol, ChatResponse, ChatMessage


            # Any class implementing the required methods is compatible
            class CustomChatClient:
                async def get_response(self, messages, **kwargs):
                    # Your custom implementation
                    return ChatResponse(messages=[], response_id="custom")

                def get_streaming_response(self, messages, **kwargs):
                    async def _stream():
                        from agent_framework import ChatResponseUpdate

                        yield ChatResponseUpdate()

                    return _stream()


            # Verify the instance satisfies the protocol
            client = CustomChatClient()
            assert isinstance(client, ChatClientProtocol)
    """

    async def get_response(
        self,
        messages: str | ChatMessage | list[str] | list[ChatMessage],
        **kwargs: Unpack[TOptions],
    ) -> ChatResponse:
        """Send input and return the response.

        Args:
            messages: The sequence of input messages to send.
            **kwargs: Chat options. See BaseChatOptionsDict for common options.

        Returns:
            The response messages generated by the client.

        Raises:
            ValueError: If the input message sequence is ``None``.
        """
        ...

    def get_streaming_response(
        self,
        messages: str | ChatMessage | list[str] | list[ChatMessage],
        **kwargs: Unpack[TOptions],
    ) -> AsyncIterable[ChatResponseUpdate]:
        """Send input messages and stream the response.

        Args:
            messages: The sequence of input messages to send.
            **kwargs: Chat options. See BaseChatOptionsDict for common options.

        Yields:
            ChatResponseUpdate: Partial response updates as they're generated.
        """
        ...


# endregion


# region ChatClientBase


class BaseChatClient(SerializationMixin, ABC, Generic[TOptions]):
    """Base class for chat clients.

    This abstract base class provides core functionality for chat client implementations,
    including middleware support, message preparation, and tool normalization.

    The generic type parameter TOptions specifies which options TypedDict this client
    accepts. This enables IDE autocomplete and type checking for provider-specific options
    when using the typed overloads of get_response and get_streaming_response.

    Note:
        BaseChatClient cannot be instantiated directly as it's an abstract base class.
        Subclasses must implement ``_inner_get_response()`` and ``_inner_get_streaming_response()``.

    Examples:
        .. code-block:: python

            from agent_framework import BaseChatClient, ChatResponse, ChatMessage
            from collections.abc import AsyncIterable


            class CustomChatClient(BaseChatClient):
                async def _inner_get_response(self, *, messages, chat_options, **kwargs):
                    # Your custom implementation
                    return ChatResponse(
                        messages=[ChatMessage(role="assistant", text="Hello!")], response_id="custom-response"
                    )

                async def _inner_get_streaming_response(self, *, messages, chat_options, **kwargs):
                    # Your custom streaming implementation
                    from agent_framework import ChatResponseUpdate

                    yield ChatResponseUpdate(role="assistant", contents=[{"type": "text", "text": "Hello!"}])


            # Create an instance of your custom client
            client = CustomChatClient()

            # Use the client to get responses
            response = await client.get_response("Hello, how are you?")
    """

    OTEL_PROVIDER_NAME: ClassVar[str] = "unknown"
    DEFAULT_EXCLUDE: ClassVar[set[str]] = {"additional_properties"}
    # This is used for OTel setup, should be overridden in subclasses

    def __init__(
        self,
        *,
        middleware: (
            Sequence[ChatMiddleware | ChatMiddlewareCallable | FunctionMiddleware | FunctionMiddlewareCallable] | None
        ) = None,
        additional_properties: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> None:
        """Initialize a BaseChatClient instance.

        Keyword Args:
            middleware: Middleware for the client.
            additional_properties: Additional properties for the client.
            kwargs: Additional keyword arguments (merged into additional_properties).
        """
        # Merge kwargs into additional_properties
        self.additional_properties = additional_properties or {}
        self.additional_properties.update(kwargs)

        self.middleware = middleware

        self.function_invocation_configuration = (
            FunctionInvocationConfiguration() if hasattr(self.__class__, FUNCTION_INVOKING_CHAT_CLIENT_MARKER) else None
        )

    def to_dict(self, *, exclude: set[str] | None = None, exclude_none: bool = True) -> dict[str, Any]:
        """Convert the instance to a dictionary.

        Extracts additional_properties fields to the root level.

        Keyword Args:
            exclude: Set of field names to exclude from serialization.
            exclude_none: Whether to exclude None values from the output. Defaults to True.

        Returns:
            Dictionary representation of the instance.
        """
        # Get the base dict from SerializationMixin
        result = super().to_dict(exclude=exclude, exclude_none=exclude_none)

        # Extract additional_properties to root level
        if self.additional_properties:
            result.update(self.additional_properties)

        return result

    @staticmethod
    async def _normalize_tools(
        tools: ToolProtocol
        | MutableMapping[str, Any]
        | Callable[..., Any]
        | Sequence[ToolProtocol | MutableMapping[str, Any] | Callable[..., Any]]
        | None = None,
    ) -> list[ToolProtocol | dict[str, Any] | Callable[..., Any]]:
        """Normalize tools input to a consistent list format.

        Expands MCP tools to their constituent functions, connecting them if needed.

        Args:
            tools: The tools in various supported formats.

        Returns:
            A normalized list of tools.
        """
        from typing import cast

        final_tools: list[ToolProtocol | dict[str, Any] | Callable[..., Any]] = []
        if not tools:
            return final_tools
        # Use cast when a sequence is passed (likely already a list)
        tools_list = (
            cast(list[ToolProtocol | MutableMapping[str, Any] | Callable[..., Any]], tools)
            if isinstance(tools, Sequence) and not isinstance(tools, (str, bytes))
            else [tools]
        )
        for tool in tools_list:  # type: ignore[reportUnknownType]
            from ._mcp import MCPTool

            if isinstance(tool, MCPTool):
                if not tool.is_connected:
                    await tool.connect()
                final_tools.extend(tool.functions)  # type: ignore
                continue
            final_tools.append(tool)  # type: ignore
        return final_tools

    # region Internal methods to be implemented by the derived classes

    @abstractmethod
    async def _inner_get_response(
        self,
        *,
        messages: MutableSequence[ChatMessage],
        options: dict[str, Any],
        **kwargs: Any,
    ) -> ChatResponse:
        """Send a chat request to the AI service.

        Keyword Args:
            messages: The chat messages to send.
            options: The options dict for the request.
            kwargs: Any additional keyword arguments.

        Returns:
            The chat response contents representing the response(s).
        """

    @abstractmethod
    async def _inner_get_streaming_response(
        self,
        *,
        messages: MutableSequence[ChatMessage],
        options: dict[str, Any],
        **kwargs: Any,
    ) -> AsyncIterable[ChatResponseUpdate]:
        """Send a streaming chat request to the AI service.

        Keyword Args:
            messages: The chat messages to send.
            options: The options dict for the request.
            kwargs: Any additional keyword arguments.

        Yields:
            ChatResponseUpdate: The streaming chat message contents.
        """
        # Below is needed for mypy: https://mypy.readthedocs.io/en/stable/more_types.html#asynchronous-iterators
        if False:
            yield
        await asyncio.sleep(0)  # pragma: no cover
        # This is a no-op, but it allows the method to be async and return an AsyncIterable.
        # The actual implementation should yield ChatResponseUpdate instances as needed.

    # endregion

    # region Public method

    async def get_response(
        self,
        messages: str | ChatMessage | list[str] | list[ChatMessage],
        **kwargs: Unpack[TOptions],
    ) -> ChatResponse:
        """Get a response from a chat client.

        Args:
            messages: The message or messages to send to the model.
            **kwargs: Chat options. See BaseChatOptionsDict for common options.
                Provider-specific options are documented in each client's TypedDict.

        Returns:
            A chat response from the model.
        """
        options: dict[str, Any] = dict(kwargs)  # type: ignore[arg-type]
        await self._prepare_tool_options(options)

        return await self._inner_get_response(messages=prepare_messages(messages), options=options)

    async def get_streaming_response(
        self,
        messages: str | ChatMessage | list[str] | list[ChatMessage],
        **kwargs: Unpack[TOptions],
    ) -> AsyncIterable[ChatResponseUpdate]:
        """Get a streaming response from a chat client.

        Args:
            messages: The message or messages to send to the model.
            **kwargs: Chat options. See BaseChatOptionsDict for common options.
                Provider-specific options are documented in each client's TypedDict.

        Yields:
            ChatResponseUpdate: A stream representing the response(s) from the LLM.
        """
        options: dict[str, Any] = dict(kwargs)  # type: ignore[arg-type]
        await self._prepare_tool_options(options)

        async for update in self._inner_get_streaming_response(messages=prepare_messages(messages), options=options):
            yield update

    async def _prepare_tool_options(self, options: dict[str, Any]) -> None:
        """Prepare tool-related options in the options dict.

        Normalizes tools (including MCP expansion) and handles tool_choice defaults.
        This method can be overridden by subclasses to customize tool handling.

        Args:
            options: The options dict to prepare (modified in place).
        """
        options["tools"] = await self._normalize_tools(options.get("tools"))
        options.setdefault("tool_choice", "auto")

        tool_choice = options.get("tool_choice")
        tools = options.get("tools")

        if tool_choice == ToolMode.NONE or tool_choice == "none":
            options.pop("tools", None)
            options["tool_choice"] = ToolMode.NONE
            return
        if not tools:
            options["tool_choice"] = ToolMode.NONE
        elif tool_choice is None:
            options["tool_choice"] = ToolMode.AUTO
        # else: keep tool_choice as-is

    def service_url(self) -> str:
        """Get the URL of the service.

        Override this in the subclass to return the proper URL.
        If the service does not have a URL, return None.

        Returns:
            The service URL or 'Unknown' if not implemented.
        """
        return "Unknown"

    def create_agent(
        self,
        *,
        id: str | None = None,
        name: str | None = None,
        description: str | None = None,
        instructions: str | None = None,
        chat_message_store_factory: Callable[[], ChatMessageStoreProtocol] | None = None,
        context_provider: ContextProvider | None = None,
        middleware: Sequence[Middleware] | None = None,
        allow_multiple_tool_calls: bool | None = None,
        conversation_id: str | None = None,
        frequency_penalty: float | None = None,
        logit_bias: dict[str | int, float] | None = None,
        max_tokens: int | None = None,
        metadata: dict[str, Any] | None = None,
        model_id: str | None = None,
        presence_penalty: float | None = None,
        response_format: type[BaseModel] | None = None,
        seed: int | None = None,
        stop: str | Sequence[str] | None = None,
        store: bool | None = None,
        temperature: float | None = None,
        tool_choice: ToolMode | Literal["auto", "required", "none"] | dict[str, Any] | None = "auto",
        tools: ToolProtocol
        | Callable[..., Any]
        | MutableMapping[str, Any]
        | Sequence[ToolProtocol | Callable[..., Any] | MutableMapping[str, Any]]
        | None = None,
        top_p: float | None = None,
        user: str | None = None,
        additional_chat_options: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> "ChatAgent":
        """Create a ChatAgent with this client.

        This is a convenience method that creates a ChatAgent instance with this
        chat client already configured.

        Keyword Args:
            id: The unique identifier for the agent. Will be created automatically if not provided.
            name: The name of the agent.
            description: A brief description of the agent's purpose.
            instructions: Optional instructions for the agent.
                These will be put into the messages sent to the chat client service as a system message.
            chat_message_store_factory: Factory function to create an instance of ChatMessageStoreProtocol.
                If not provided, the default in-memory store will be used.
            context_provider: Context provider to include during agent invocation.
            middleware: List of middleware to intercept chat and function invocations.
            allow_multiple_tool_calls: Whether to allow multiple tool calls per agent turn.
            conversation_id: The conversation ID to associate with the agent's messages.
            frequency_penalty: The frequency penalty to use.
            logit_bias: The logit bias to use.
            max_tokens: The maximum number of tokens to generate.
            metadata: Additional metadata to include in the request.
            model_id: The model_id to use for the agent.
            presence_penalty: The presence penalty to use.
            response_format: The format of the response.
            seed: The random seed to use.
            stop: The stop sequence(s) for the request.
            store: Whether to store the response.
            temperature: The sampling temperature to use.
            tool_choice: The tool choice for the request.
            tools: The tools to use for the request.
            top_p: The nucleus sampling probability to use.
            user: The user to associate with the request.
            additional_chat_options: A dictionary of other values that will be passed through
                to the chat_client ``get_response`` and ``get_streaming_response`` methods.
                This can be used to pass provider specific parameters.
            kwargs: Any additional keyword arguments. Will be stored as ``additional_properties``.

        Returns:
            A ChatAgent instance configured with this chat client.

        Examples:
            .. code-block:: python

                from agent_framework.clients import OpenAIChatClient

                # Create a client
                client = OpenAIChatClient(model_id="gpt-4")

                # Create an agent using the convenience method
                agent = client.create_agent(
                    name="assistant", instructions="You are a helpful assistant.", temperature=0.7
                )

                # Run the agent
                response = await agent.run("Hello!")
        """
        from ._agents import ChatAgent

        return ChatAgent(
            chat_client=self,
            id=id,
            name=name,
            description=description,
            instructions=instructions,
            chat_message_store_factory=chat_message_store_factory,
            context_provider=context_provider,
            middleware=middleware,
            allow_multiple_tool_calls=allow_multiple_tool_calls,
            conversation_id=conversation_id,
            frequency_penalty=frequency_penalty,
            logit_bias=logit_bias,
            max_tokens=max_tokens,
            metadata=metadata,
            model_id=model_id,
            presence_penalty=presence_penalty,
            response_format=response_format,
            seed=seed,
            stop=stop,
            store=store,
            temperature=temperature,
            tool_choice=tool_choice,
            tools=tools,
            top_p=top_p,
            user=user,
            additional_chat_options=additional_chat_options,
            **kwargs,
        )
