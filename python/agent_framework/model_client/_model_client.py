# Copyright (c) Microsoft. All rights reserved.

from __future__ import annotations

from typing import AsyncIterable, List, Protocol, Sequence

from agent_framework import CancellationToken, InputGuardrail, OutputGuardrail
from agent_framework.model_client import ChatMessage, ChatResponse, ChatResponseUpdate


class ModelClient(Protocol):
    """A protocol for a model client that can generate chat responses."""
    async def generate_response(
        self,
        messages: Sequence[ChatMessage],
        *,
        cancellation_token: CancellationToken | None = None,
        **options,  # kwargs?
    ) -> ChatResponse:
        """Sends chat messages and returns the response.

        Args:
            messages: The sequence of chat messages to send.
            cancellation_token: The `CancellationToken` to monitor for cancellation requests.
            **options: Additional options for the chat request, such as model_id, temperature, etc.
                       See `ChatOptions` for more details.

        Returns:
            The response messages generated by the client.

        Raises:
            ValueError: If the input message sequence is `None`.
        """
        ...

    async def generate_streaming_response(
        self,
        messages: Sequence[ChatMessage],
        *,
        cancellation_token: CancellationToken | None = None,
        **options,  # kwargs?
    ) -> AsyncIterable[ChatResponseUpdate]:
        """Sends chat messages and streams the response.

        Args:
            messages: The sequence of chat messages to send.
            cancellation_token: The `CancellationToken` to monitor for cancellation requests.
            **options: Additional options for the chat request, such as model_id, temperature, etc.
                       See `ChatOptions` for more details.

        Returns:
            An async iterable of chat response updates containing the content of the response messages
            generated by the client.

        Raises:
            ValueError: If the input message sequence is `None`.
        """
        ...

    def add_input_guardrails(
        self,
        guardrails: List[InputGuardrail[ChatMessage]]
    ) -> None:
        """Add input guardrails to the model client.

        Args:
            guardrails: The list of input guardrails to add.
        """
        ...

    def add_output_guardrails(
        self,
        guardrails: List[OutputGuardrail[ChatResponse | Sequence[ChatResponseUpdate]]]
    ) -> None:
        """Add output guardrails to the model client.

        Args:
            guardrails: The list of output guardrails to add.
        """
        ...

# Note that a lot of additional functionality in M.E.AI is provided by typed-wrappers, e.g.:
# - FunctionInvokingModelClient
# - LoggingModelClient
# - DelegatingModelClient
# - DistributedCachingModelClient
# This functionality is going to be provided at the Agent level, not at the ModelClient level.
